{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Logic Synthesis for AI\n",
    "--------------------\n",
    "\n",
    "Logic synthesis is a crucial process in digital circuit design where a high-level description of a system's behavior is transformed into a gate-level representation. In the context of AI, particularly deep learning, logic synthesis enables the efficient implementation of neural networks on hardware such as FPGAs and ASICs.\n",
    "\n",
    "This notebook uses the NNgen library to demonstrate how deep neural network models can be synthesized into hardware. NNgen is a versatile tool that converts neural network descriptions into hardware description languages like Verilog. This is essential for developing custom hardware accelerators that significantly enhance the performance and efficiency of AI models.\n",
    "\n",
    "Understanding how to synthesize AI models into hardware is vital for advancing AI applications, particularly in areas requiring high performance and low power consumption, such as embedded systems, edge computing, and real-time data processing.\n",
    "\n",
    "This notebook is based on the [NNgen tutorial](https://github.com/NNgen/nngen).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": "pip install nngen onnx numpy veriloggen",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Architecture with NNgen\n",
    "\n",
    "In this section, we define a simple deep neural network (DNN) model using the NNgen library.\n",
    "\n",
    "The network consists of:\n",
    "\n",
    "1. **Input Layer**: Defined as a placeholder for input data with dimensions 32x32x3 (e.g., an RGB image) and a batch size of 1.\n",
    "\n",
    "2. **First Convolutional Layer**:\n",
    "   - Weights (`w0`), biases (`b0`), and scales (`s0`) are initialized.\n",
    "   - A convolution operation (`conv2d`) is applied to the input, followed by ReLU activation and max pooling.\n",
    "\n",
    "3. **Second Convolutional Layer**:\n",
    "   - Similar to the first layer, with new weights (`w1`), biases (`b1`), and scales (`s1`).\n",
    "   - The output is reshaped to prepare for fully connected layers.\n",
    "\n",
    "4. **First Fully Connected Layer**:\n",
    "   - Weights (`w2`), biases (`b2`), and scales (`s2`) are defined.\n",
    "   - A matrix multiplication (`matmul`) is performed with ReLU activation.\n",
    "\n",
    "5. **Second Fully Connected Layer**:\n",
    "   - New weights (`w3`), biases (`b3`), and scales (`s3`) are set.\n",
    "   - The final matrix multiplication operation produces the output layer.\n",
    "\n",
    "This structure showcases the typical layers found in a convolutional neural network (CNN) and highlights how NNgen can be used to define each layer's operations and data types, preparing the model for efficient hardware synthesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import nngen as ng\n",
    "\n",
    "\n",
    "# data types\n",
    "act_dtype = ng.int8\n",
    "weight_dtype = ng.int8\n",
    "bias_dtype = ng.int32\n",
    "scale_dtype = ng.int8\n",
    "batchsize = 1\n",
    "\n",
    "# input\n",
    "input_layer = ng.placeholder(dtype=act_dtype,\n",
    "                             shape=(batchsize, 32, 32, 3),  # N, H, W, C\n",
    "                             name='input_layer')\n",
    "\n",
    "# layer 0: conv2d (with bias and scale (= batchnorm)), relu, max_pool\n",
    "w0 = ng.variable(dtype=weight_dtype,\n",
    "                 shape=(64, 3, 3, 3),  # Och, Ky, Kx, Ich\n",
    "                 name='w0')\n",
    "b0 = ng.variable(dtype=bias_dtype,\n",
    "                 shape=(w0.shape[0],), name='b0')\n",
    "s0 = ng.variable(dtype=scale_dtype,\n",
    "                 shape=(w0.shape[0],), name='s0')\n",
    "\n",
    "a0 = ng.conv2d(input_layer, w0,\n",
    "               strides=(1, 1, 1, 1),\n",
    "               bias=b0,\n",
    "               scale=s0,\n",
    "               act_func=ng.relu,\n",
    "               dtype=act_dtype,\n",
    "               sum_dtype=ng.int32)\n",
    "\n",
    "a0p = ng.max_pool_serial(a0,\n",
    "                         ksize=(1, 2, 2, 1),\n",
    "                         strides=(1, 2, 2, 1))\n",
    "\n",
    "# layer 1: conv2d, relu, reshape\n",
    "w1 = ng.variable(weight_dtype,\n",
    "                 shape=(64, 3, 3, a0.shape[-1]),\n",
    "                 name='w1')\n",
    "b1 = ng.variable(bias_dtype,\n",
    "                 shape=(w1.shape[0],),\n",
    "                 name='b1')\n",
    "s1 = ng.variable(scale_dtype,\n",
    "                 shape=(w1.shape[0],),\n",
    "                 name='s1')\n",
    "\n",
    "a1 = ng.conv2d(a0p, w1,\n",
    "               strides=(1, 1, 1, 1),\n",
    "               bias=b1,\n",
    "               scale=s1,\n",
    "               act_func=ng.relu,\n",
    "               dtype=act_dtype,\n",
    "               sum_dtype=ng.int32)\n",
    "\n",
    "a1r = ng.reshape(a1, [batchsize, -1])\n",
    "\n",
    "# layer 2: full-connection, relu\n",
    "w2 = ng.variable(weight_dtype,\n",
    "                 shape=(256, a1r.shape[-1]),\n",
    "                 name='w2')\n",
    "b2 = ng.variable(bias_dtype,\n",
    "                 shape=(w2.shape[0],),\n",
    "                 name='b2')\n",
    "s2 = ng.variable(scale_dtype,\n",
    "                 shape=(w2.shape[0],),\n",
    "                 name='s2')\n",
    "\n",
    "a2 = ng.matmul(a1r, w2,\n",
    "               bias=b2,\n",
    "               scale=s2,\n",
    "               transposed_b=True,\n",
    "               act_func=ng.relu,\n",
    "               dtype=act_dtype,\n",
    "               sum_dtype=ng.int32)\n",
    "\n",
    "# layer 3: full-connection, relu\n",
    "w3 = ng.variable(weight_dtype,\n",
    "                 shape=(10, a2.shape[-1]),\n",
    "                 name='w3')\n",
    "b3 = ng.variable(bias_dtype,\n",
    "                 shape=(w3.shape[0],),\n",
    "                 name='b3')\n",
    "s3 = ng.variable(scale_dtype,\n",
    "                 shape=(w3.shape[0],),\n",
    "                 name='s3')\n",
    "\n",
    "# output\n",
    "output_layer = ng.matmul(a2, w3,\n",
    "                         bias=b3,\n",
    "                         scale=s3,\n",
    "                         transposed_b=True,\n",
    "                         name='output_layer',\n",
    "                         dtype=act_dtype,\n",
    "                         sum_dtype=ng.int32)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Alternative) Import an Existing Model on a DNN Framework via ONNX\n",
    "\n",
    "Instead of explicit model construction, you can import an existing model via the ONNX-importer. This allows for leveraging pre-trained models from, e.g., [`torchvision`](https://pytorch.org/vision/stable/index.html).\n",
    "\n",
    "Then, translate the model into an ONNX file, which can be imported as an NNgen model definition using the `ng.from_onnx` method.\n",
    "\n",
    "Here's a brief example:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Download a pre-trained model from Torchvision\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# PyTorch to ONNX\n",
    "onnx_filename = 'resnet18_imagenet.onnx'\n",
    "dummy_input = torch.randn(*act_shape).transpose(1, 3)\n",
    "input_names = ['act']\n",
    "output_names = ['out']\n",
    "model.eval()\n",
    "torch.onnx.export(model, dummy_input, onnx_filename,\n",
    "                  input_names=input_names, output_names=output_names)\n",
    "\n",
    "# ONNX to NNgen\n",
    "dtypes = {}\n",
    "(outputs, placeholders, variables,\n",
    " constants, operators) = ng.from_onnx(onnx_filename,\n",
    "                                      value_dtypes=dtypes,\n",
    "                                      default_placeholder_dtype=act_dtype,\n",
    "                                      default_variable_dtype=weight_dtype,\n",
    "                                      default_constant_dtype=weight_dtype,\n",
    "                                      default_operator_dtype=act_dtype,\n",
    "                                      default_scale_dtype=scale_dtype,\n",
    "                                      default_bias_dtype=bias_dtype,\n",
    "                                      disable_fusion=disable_fusion)\n",
    "```\n",
    "*⚠️ Note: This snipped is intended as a pointer for your self-study after the tutorial. It is not executable by default because you would have to modify the subsequent code to match the imported model!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning Random Values and Quantizing Weights\n",
    "\n",
    "In this example, we assign random floating-point values to the network weights and biases for demonstration purposes. In a real-world scenario, you would use actual trained weight values from a deep neural network framework.\n",
    "\n",
    "### Weight Initialization\n",
    "We initialize the weights and biases with random values clipped between -3.0 and 3.0. Scales are set to 1. This is done for all layers of the network.\n",
    "\n",
    "### Quantization\n",
    "\n",
    "To prepare the model for hardware synthesis, we use NNgen’s quantizer to convert these floating-point weights to integers. This process involves:\n",
    "\n",
    "- Setting scale factors based on activation data type width.\n",
    "- Normalizing the input data using ImageNet mean and standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_weight(shape):\n",
    "    value = np.random.normal(size=np.prod(shape)).reshape(shape)\n",
    "    value = np.clip(value, -3.0, 3.0)\n",
    "    return value\n",
    "\n",
    "def initialize_bias(shape):\n",
    "    value = np.random.normal(size=np.prod(shape)).reshape(shape)\n",
    "    value = np.clip(value, -3.0, 3.0)\n",
    "    return value\n",
    "\n",
    "def initialize_scale(shape):\n",
    "    return np.ones(shape)\n",
    "\n",
    "def set_values(variables, init_function):\n",
    "    for var in variables:\n",
    "        var.set_value(init_function(var.shape))\n",
    "\n",
    "# Initialize weights\n",
    "weights = [w0, w1, w2, w3]\n",
    "set_values(weights, initialize_weight)\n",
    "\n",
    "# Initialize biases\n",
    "biases = [b0, b1, b2, b3]\n",
    "set_values(biases, initialize_bias)\n",
    "\n",
    "# Initialize scales\n",
    "scales = [s0, s1, s2, s3]\n",
    "set_values(scales, initialize_scale)\n",
    "\n",
    "# Quantizing the floating-point weights using the NNgen quantizer\n",
    "imagenet_mean = np.array([0.485, 0.456, 0.406]).astype(np.float32)\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225]).astype(np.float32)\n",
    "\n",
    "act_scale_factor = 128 if act_dtype.width > 8 else int(round(2 ** (act_dtype.width - 1) * 0.5))\n",
    "\n",
    "input_scale_factors = {'input_layer': act_scale_factor}\n",
    "input_means = {'input_layer': imagenet_mean * act_scale_factor}\n",
    "input_stds = {'input_layer': imagenet_std * act_scale_factor}\n",
    "\n",
    "ng.quantize([output_layer], input_scale_factors, input_means, input_stds)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning Hardware Attributes\n",
    "\n",
    "In deep learning, models are typically executed on hardware with parallel processing capabilities to speed up computations. This is particularly important when dealing with large datasets and complex models, where performance can be a bottleneck.\n",
    "\n",
    "The code cell assigns hardware attributes to different layers of a neural network model to optimize performance. These attributes configure the degree of parallelism in various directions (input channels, output channels, pixel columns, and rows) and the right-shift amount for integer precision execution.\n",
    "\n",
    "### Key Attributes:\n",
    "\n",
    "**Parallelism**:\n",
    "- `par_ich`: Parallelism in input channels. More input channels processed simultaneously.\n",
    "- `par_och`: Parallelism in output channels. More output channels processed simultaneously.\n",
    "\n",
    "### Why It's Important:\n",
    "\n",
    "**Performance**: By configuring parallelism, the model can leverage hardware capabilities to process multiple operations simultaneously, leading to faster execution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "par_ich = 2\n",
    "par_och = 2\n",
    "\n",
    "a0.attribute(par_ich=par_ich, par_och=par_och)\n",
    "a1.attribute(par_ich=par_ich, par_och=par_och)\n",
    "a2.attribute(par_ich=par_ich, par_och=par_och)\n",
    "output_layer.attribute(par_ich=par_ich, par_och=par_och)\n",
    "\n",
    "par = par_och\n",
    "\n",
    "a0p.attribute(par=par)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the DNN Model Behavior by Executing the NNgen Dataflow as Software\n",
    "\n",
    "After assigning weight values, the constructed NNgen dataflow can be executed as software to verify the behavior of a quantized DNN model. The `ng.eval` method evaluates the NNgen dataflow according to input values provided via method arguments.\n",
    "\n",
    "In this example, random integer values are produced by NumPy and assigned as an input. However, in practice, actual integer input values, such as image data opened by PIL, should be used.\n",
    "\n",
    "### Steps:\n",
    "1. Generate Input Values:\n",
    "    - Random integer values are generated using NumPy.\n",
    "    - The values are then clipped and scaled to fit the expected range for the input layer.\n",
    "    - The values are rounded and converted to the appropriate integer type.\n",
    "\n",
    "2. Evaluate the Model:\n",
    "    - The `ng.eval` method is called with the input values to evaluate the NNgen dataflow.\n",
    "    - The output of the model is captured and printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "input_layer_value = np.random.normal(size=input_layer.length).reshape(input_layer.shape)\n",
    "input_layer_value = input_layer_value * imagenet_std + imagenet_mean\n",
    "input_layer_value = np.clip(input_layer_value, -3.0, 3.0)\n",
    "input_layer_value = input_layer_value * act_scale_factor\n",
    "input_layer_value = np.clip(input_layer_value,\n",
    "                            -1 * 2 ** (act_dtype.width - 1) - 1, 2 ** (act_dtype.width - 1))\n",
    "input_layer_value = np.round(input_layer_value).astype(np.int64)\n",
    "\n",
    "eval_outs = ng.eval([output_layer], input_layer=input_layer_value)\n",
    "output_layer_value = eval_outs[0]\n",
    "\n",
    "print(output_layer_value)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the NNgen dataflow to a hardware description (Verilog HDL and IP-XACT)\n",
    "\n",
    "After all the weights are assigned and the hardware attributes are configured, the NNgen dataflow is ready to be converted to an actual hardware description.\n",
    "\n",
    "You can specify the hardware parameters, such as a data width of the AXI interface and system-wide signal names, via the \"config\" argument. Please see \"nngen/verilog.py\" for all the list of configurable hardware parameters.\n",
    "\n",
    "NNgen generates an all-inclusive dedicated hardware design for an input DNN model, which includes parallel processing elements, on-chip memories, on-chip network between the processing elements and the on-chip memories, a DMA controller between off-chip memories and on-chip memories, and FSM-based control circuits. Therefore, no external control, such as DMA on CPU is required after the generated hardware begins a computation.\n",
    "\n",
    "NNgen supports 3 types of output: 1) Veriloggen object, which is Python-based high-level hardware abstraction, 2) IP-XACT, which is a common IP-core format, and 3) Verilog HDL RTL as a text file.\n",
    "A generated Veriloggen object can be easily verified by a testing mechanism of Veriloggen and a Verilog simulator.\n",
    "A generated IP-XACT IP-core can be integrated with other components via AMBA AXI4 interface on an FPGA."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "silent = False\n",
    "axi_datawidth = 32\n",
    "\n",
    "# to Veriloggen object\n",
    "# targ = ng.to_veriloggen([output_layer], 'dnn_accelerator', silent=silent,\n",
    "#                        config={'maxi_datawidth': axi_datawidth})\n",
    "\n",
    "# to IP-XACT (the method returns Veriloggen object, as well as to_veriloggen)\n",
    "targ = ng.to_ipxact([output_layer], 'dnn_accelerator', silent=silent,\n",
    "                    config={'maxi_datawidth': axi_datawidth})\n",
    "print('# IP-XACT was generated. Check the current directory.')\n",
    "\n",
    "# to Verilog HDL RTL (the method returns a source code text)\n",
    "# rtl = ng.to_verilog([output_layer], 'dnn_accelerator', silent=silent,\n",
    "#                    config={'maxi_datawidth': axi_datawidth})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the quantized weights\n",
    "\n",
    "All weight parameters are zipped into a single `np.ndarray` by the `ng.export_ndarray` method. This array can be utilized in an actual FPGA platform."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# convert weight values to a memory image:\n",
    "# on a real FPGA platform, this image will be used as a part of the model definition.\n",
    "param_filename = 'dnn_accelerator.npz'\n",
    "chunk_size = 64\n",
    "\n",
    "param_data = ng.export_ndarray([output_layer], chunk_size)\n",
    "np.savez_compressed(param_filename, param_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate the generated hardware by Veriloggen and Verilog simulator\n",
    "\n",
    "If you want to reduce the development time, you can skip this section for Verilog simulation.\n",
    "\n",
    "If you generate a hardware as Veriloggen object or IP-XACT, you can simulate the hardware behavior on Verilog simulator via the testing mechanism on Veriloggen.\n",
    "\n",
    "Before the hardware runs, the input data and weight values should be located on the shared off-chip memory. In Verilog simulation in the example, there is a `np.ndarray` object to represent a dump image of the off-chip memory. You can copy the pre-computed values to the memory image by `axi.set_memory` method.\n",
    "\n",
    "`param_data` is the unified parameter data of all variables and constants. Locations of the located data are configurable, which can be changed from the CPU via the configuration register of the NNgen hardware. In the following example, the head address of unified parameter data (`variable_addr`) is calculated by the same rule as the address calculator in the NNgen compiler.\n",
    "\n",
    "The `ctrl` method in the following example is an emulation of a control program on the CPU, which is actually an FSM circuit of the control sequence synthesized by the procedural high-level synthesis compiler of Veriloggen. Via the `ng.sim.start` method, the program writes `1` to the `start` register of the NNgen hardware. Then, the hardware begins the computation, and the CPU waits until the computation finishes via the `ng.sim.wait` method.\n",
    "\n",
    "### Data alignment, `word_alignment`, and `aligned_shape`\n",
    "\n",
    "**Note that all the input, weight, and output data should be located along with their alignments.** Especially, using a narrower data width (for any data) than the AXI interconnect interface and applying the parallelization via the hardware attribute will require special cares of data arrangement. In a synthesis log, you can find the `word_alignment` and `aligned_shape` for each placeholder, variable, operator. When putting corresponding data on an off-chip memory, a padding will be required according to the word alignment. The difference between the original shape and the aligned shape is the size of padding. In NNgen, padding is required only at an innermost dimension.\n",
    "\n",
    "Unified variable images, such as `param_data`, are already aligned according to the word alignment. So you don't have to rearrange the data alignment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import math\n",
    "from veriloggen import *\n",
    "import veriloggen.thread as vthread\n",
    "import veriloggen.types.axi as axi\n",
    "\n",
    "outputfile = 'dnn_accelerator.out'\n",
    "filename = 'dnn_accelerator.v'\n",
    "# simtype = 'iverilog'\n",
    "simtype = 'verilator'\n",
    "\n",
    "param_bytes = len(param_data)\n",
    "\n",
    "variable_addr = int(\n",
    "    math.ceil((input_layer.addr + input_layer.memory_size) / chunk_size)) * chunk_size\n",
    "check_addr = int(math.ceil((variable_addr + param_bytes) / chunk_size)) * chunk_size\n",
    "tmp_addr = int(math.ceil((check_addr + output_layer.memory_size) / chunk_size)) * chunk_size\n",
    "\n",
    "memimg_datawidth = 32\n",
    "mem = np.zeros([1024 * 1024 * 256 // memimg_datawidth], dtype=np.int64)\n",
    "mem = mem + [100]\n",
    "\n",
    "# placeholder\n",
    "axi.set_memory(mem, input_layer_value, memimg_datawidth,\n",
    "               act_dtype.width, input_layer.addr,\n",
    "               max(int(math.ceil(axi_datawidth / act_dtype.width)), par_ich))\n",
    "\n",
    "# parameters (variable and constant)\n",
    "axi.set_memory(mem, param_data, memimg_datawidth,\n",
    "               8, variable_addr)\n",
    "\n",
    "# verification data\n",
    "axi.set_memory(mem, output_layer_value, memimg_datawidth,\n",
    "               act_dtype.width, check_addr,\n",
    "               max(int(math.ceil(axi_datawidth / act_dtype.width)), par_och))\n",
    "\n",
    "# test controller\n",
    "m = Module('test')\n",
    "params = m.copy_params(targ)\n",
    "ports = m.copy_sim_ports(targ)\n",
    "clk = ports['CLK']\n",
    "resetn = ports['RESETN']\n",
    "rst = m.Wire('RST')\n",
    "rst.assign(Not(resetn))\n",
    "\n",
    "# AXI memory model\n",
    "if outputfile is None:\n",
    "    outputfile = os.path.splitext(os.path.basename(__file__))[0] + '.out'\n",
    "\n",
    "memimg_name = 'memimg_' + outputfile\n",
    "\n",
    "memory = axi.AxiMemoryModel(m, 'memory', clk, rst,\n",
    "                            datawidth=axi_datawidth,\n",
    "                            memimg=mem, memimg_name=memimg_name,\n",
    "                            memimg_datawidth=memimg_datawidth)\n",
    "memory.connect(ports, 'maxi')\n",
    "\n",
    "# AXI-Slave controller\n",
    "_saxi = vthread.AXIMLite(m, '_saxi', clk, rst, noio=True)\n",
    "_saxi.connect(ports, 'saxi')\n",
    "\n",
    "# timer\n",
    "time_counter = m.Reg('time_counter', 32, initval=0)\n",
    "seq = Seq(m, 'seq', clk, rst)\n",
    "seq(\n",
    "    time_counter.inc()\n",
    ")\n",
    "\n",
    "\n",
    "def ctrl():\n",
    "    for i in range(100):\n",
    "        pass\n",
    "\n",
    "    ng.sim.set_global_addrs(_saxi, tmp_addr)\n",
    "\n",
    "    start_time = time_counter.value\n",
    "    ng.sim.start(_saxi)\n",
    "\n",
    "    print('# start')\n",
    "\n",
    "    ng.sim.wait(_saxi)\n",
    "    end_time = time_counter.value\n",
    "\n",
    "    print('# end')\n",
    "    print('# execution cycles: %d' % (end_time - start_time))\n",
    "\n",
    "    # verify\n",
    "    ok = True\n",
    "    for bat in range(output_layer.shape[0]):\n",
    "        for x in range(output_layer.shape[1]):\n",
    "            orig = memory.read_word(bat * output_layer.aligned_shape[1] + x,\n",
    "                                    output_layer.addr, act_dtype.width)\n",
    "            check = memory.read_word(bat * output_layer.aligned_shape[1] + x,\n",
    "                                     check_addr, act_dtype.width)\n",
    "\n",
    "            if vthread.verilog.NotEql(orig, check):\n",
    "                print('NG (', bat, x,\n",
    "                      ') orig: ', orig, ' check: ', check)\n",
    "                ok = False\n",
    "            else:\n",
    "                print('OK (', bat, x,\n",
    "                      ') orig: ', orig, ' check: ', check)\n",
    "\n",
    "    if ok:\n",
    "        print('# verify: PASSED')\n",
    "    else:\n",
    "        print('# verify: FAILED')\n",
    "\n",
    "    vthread.finish()\n",
    "\n",
    "\n",
    "th = vthread.Thread(m, 'th_ctrl', clk, rst, ctrl)\n",
    "fsm = th.start()\n",
    "\n",
    "uut = m.Instance(targ, 'uut',\n",
    "                 params=m.connect_params(targ),\n",
    "                 ports=m.connect_ports(targ))\n",
    "\n",
    "# simulation.setup_waveform(m, uut)\n",
    "simulation.setup_clock(m, clk, hperiod=5)\n",
    "init = simulation.setup_reset(m, resetn, m.make_reset(), period=100, polarity='low')\n",
    "\n",
    "init.add(\n",
    "    Delay(10000000),\n",
    "    Systask('finish'),\n",
    ")\n",
    "\n",
    "# output source code\n",
    "if filename is not None:\n",
    "    m.to_verilog(filename)\n",
    "\n",
    "# run simulation\n",
    "sim = simulation.Simulator(m, sim=simtype)\n",
    "rslt = sim.run(outputfile=outputfile)\n",
    "\n",
    "print(rslt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Next Steps: From Verilog to FPGA Implementation\n",
    "\n",
    "After successfully synthesizing your neural network into Verilog and simulating its behavior, the next logical step is to implement this design on an FPGA. This process involves several stages, often referred to as the \"placement and routing\" flow.\n",
    "\n",
    "A powerful open-source toolchain for this is [**Verilog to Routing (VTR)**](https://verilogtorouting.org/). The VTR project takes a Verilog description of a digital circuit and a description of the target FPGA architecture, and performs:\n",
    "1.  **Synthesis**: Although NNgen has already produced Verilog, VTR can perform further synthesis steps or integrate with synthesis tools like [yosys](https://github.com/YosysHQ/yosys).\n",
    "2.  **Packing**: Groups logic blocks from the synthesized netlist into the complex logic blocks available on the target FPGA.\n",
    "3.  **Placement**: Decides the physical location of each packed logic block on the FPGA.\n",
    "4.  **Routing**: Connects the placed logic blocks using the available routing resources (wires and switches) on the FPGA.\n",
    "5.  **Bitstream Generation**: The final output is a bitstream file that can be used to configure the FPGA.\n",
    "\n",
    "### Using Verilog to Routing (VTR)\n",
    "\n",
    "To proceed with VTR, you would typically:\n",
    "1.  **Install VTR**: Follow the installation instructions on the [VTR project website](https://verilogtorouting.org/).\n",
    "2.  **Prepare Inputs**:\n",
    "    *   Your Verilog design (e.g., `dnn_accelerator.v` generated by NNgen in this tutorial).\n",
    "    *   An FPGA architecture description file. VTR includes examples for academic architectures. For commercial FPGAs, architecture details are typically handled by vendor tools like [AMD Vivado](https://www.amd.com/en/products/software/adaptive-socs-and-fpgas/vivado.html).\n",
    "3.  **Run the VTR Flow**: Execute the VTR flow scripts, providing your Verilog and architecture files as inputs.\n",
    "\n",
    "By using VTR, you can take your synthesized hardware design from NNgen and move towards a physical implementation on an FPGA, enabling real-world acceleration of your AI models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
